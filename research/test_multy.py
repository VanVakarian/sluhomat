import os
import modal
from pathlib import Path
import time

MODEL_DIR = "/model"
MODEL_NAME = "openai/whisper-large-v3"
MODEL_REVISION = "afda370583db9c5359511ed5d989400a6199dfe1"

image = (
    modal.Image.debian_slim(python_version="3.11")
    .apt_install("ffmpeg")
    .pip_install(
        "torch==2.1.2",
        "transformers==4.39.3",
        "hf-transfer==0.1.6",
        "huggingface_hub==0.22.2",
        "librosa==0.10.2",
        "soundfile==0.12.1",
        "accelerate==0.33.0",
        "numpy==1.24.3",
        "pydub==0.25.1",
    )
    .env({"HF_HUB_ENABLE_HF_TRANSFER": "1"})
)

app = modal.App("whisper-batch-inference", image=image)


@app.cls(
    gpu="a10g",
    concurrency_limit=10,
)
class Model:
    @modal.build()
    def download_model(self):
        from huggingface_hub import snapshot_download
        from transformers.utils import move_cache
        os.makedirs(MODEL_DIR, exist_ok=True)

        snapshot_download(
            MODEL_NAME,
            local_dir=MODEL_DIR,
            ignore_patterns=["*.pt", "*.bin"],
            revision=MODEL_REVISION,
        )
        move_cache()

    @modal.enter()
    def load_model(self):
        import torch
        from transformers import (
            AutoModelForSpeechSeq2Seq,
            AutoProcessor,
            pipeline,
        )
        self.processor = AutoProcessor.from_pretrained(MODEL_NAME)
        self.model = AutoModelForSpeechSeq2Seq.from_pretrained(
            MODEL_NAME,
            torch_dtype=torch.float16,
            low_cpu_mem_usage=True,
            use_safetensors=True,
        ).to("cuda")
        self.model.generation_config.language = "<|ru|>"
        self.pipeline = pipeline(
            "automatic-speech-recognition",
            model=self.model,
            tokenizer=self.processor.tokenizer,
            feature_extractor=self.processor.feature_extractor,
            torch_dtype=torch.float16,
            device="cuda",
        )

    @modal.batched(max_batch_size=64, wait_ms=1000)
    def transcribe(self, audio_samples):
        import time
        import numpy as np
        start = time.monotonic_ns()
        print(f"–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ–º {len(audio_samples)} –∞—É–¥–∏–æ —Ñ–∞–π–ª–æ–≤")

        if isinstance(audio_samples[0], list):
            audio_samples = [np.array(sample).squeeze() for sample in audio_samples]

        transcriptions = []
        for sample in audio_samples:
            sample = sample.squeeze()
            if len(sample.shape) > 1:
                sample = sample.mean(axis=0)

            print(f"–§–æ—Ä–º–∞ –∞—É–¥–∏–æ –ø–µ—Ä–µ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π: {sample.shape}")
            result = self.pipeline(
                sample,
                batch_size=1,
                generate_kwargs={"language": "<|ru|>"}
            )
            transcriptions.append(result)
            print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è: {result}")

        end = time.monotonic_ns()
        print(
            f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(audio_samples)} —Ñ–∞–π–ª–æ–≤ –∑–∞ {round((end - start) / 1e9, 2)}—Å"
        )
        return transcriptions


@app.function()
def transcribe_files(input_files):
    import librosa
    import numpy as np
    import io
    from pydub import AudioSegment

    model = Model()
    audio_data = []
    file_info = []

    for file_content, filename in input_files:
        try:
            # –ü—Ä–æ–±—É–µ–º —Å–Ω–∞—á–∞–ª–∞ —á–µ—Ä–µ–∑ librosa
            try:
                audio_array, sr = librosa.load(io.BytesIO(file_content), sr=16000, mono=True)
            except:
                # –ï—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å, –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —á–µ—Ä–µ–∑ pydub
                print(f"–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º {filename} —á–µ—Ä–µ–∑ pydub")
                audio = AudioSegment.from_file(io.BytesIO(file_content))
                wav_io = io.BytesIO()
                audio.export(wav_io, format='wav')
                wav_io.seek(0)
                audio_array, sr = librosa.load(wav_io, sr=16000, mono=True)

            audio_array = audio_array.squeeze()
            if len(audio_array.shape) > 1:
                audio_array = audio_array.mean(axis=0)

            if len(audio_array) > 0:
                print(f"–£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω —Ñ–∞–π–ª {filename}, —Ñ–æ—Ä–º–∞: {audio_array.shape}, —á–∞—Å—Ç–æ—Ç–∞: {sr}")
                audio_data.append(audio_array)
                file_info.append(filename)
            else:
                print(f"–û—à–∏–±–∫–∞: –ø—É—Å—Ç–æ–π –∞—É–¥–∏–æ—Ñ–∞–π–ª {filename}")
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {filename}: {e}")
            continue

    if not audio_data:
        print("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞")
        return {}

    print("üîä –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ñ–∞–π–ª—ã –Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫—É")
    print(f"–†–∞–∑–º–µ—Ä—ã —Ñ–∞–π–ª–æ–≤: {[audio.shape for audio in audio_data]}")

    transcriptions = model.transcribe.remote(audio_data)
    results = {}

    for transcription, filepath in zip(transcriptions, file_info):
        text = transcription.strip()
        print(f"–ü–æ–ª—É—á–µ–Ω–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –¥–ª—è {filepath}: {text}")
        results[filepath] = text

    return results


@app.local_entrypoint()
def main():
    start_time = time.monotonic_ns()
    print("‚è±Ô∏è –ù–∞—á–∞–ª–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è")

    audio_extensions = {'.mp3', '.wav', '.aac', '.m4a', '.flac', '.ogg'}
    audio_files = []

    for f in Path('.').glob('*'):
        if f.suffix.lower() in audio_extensions:
            try:
                with open(f, 'rb') as audio_file:
                    content = audio_file.read()
                    audio_files.append((content, str(f)))
                    print(f"–ü—Ä–æ—á–∏—Ç–∞–Ω —Ñ–∞–π–ª {f}, —Ä–∞–∑–º–µ—Ä: {len(content)} –±–∞–π—Ç")
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ {f}: {e}")

    if not audio_files:
        print("‚ö†Ô∏è –ê—É–¥–∏–æ—Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ —Ç–µ–∫—É—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏")
        end_time = time.monotonic_ns()
        print(f"‚è±Ô∏è –û–±—â–µ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {round((end_time - start_time) / 1e9, 2)}—Å")
        return

    print(f"üìÅ –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(audio_files)}")
    results = transcribe_files.remote(audio_files)

    for filepath, text in results.items():
        output_path = Path(filepath).with_suffix('.txt')
        print(f"\nüìù –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —Ñ–∞–π–ª–∞ {filepath}:")
        print(text)
        print("\n–°–æ—Ö—Ä–∞–Ω—è–µ–º –≤", output_path)

        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(text)

        print("‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ")

    end_time = time.monotonic_ns()
    print(f"‚è±Ô∏è –û–±—â–µ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {round((end_time - start_time) / 1e9, 2)}—Å")
